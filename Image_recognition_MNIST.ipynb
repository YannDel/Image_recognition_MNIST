{
 "cells": [
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "behind-manual",
   "metadata": {},
   "source": [
    "# Computer Vision - Image recognition via Deep Learning\n",
    "\n",
    "## Project's objective\n",
    "\n",
    "Identify handwritten digits with the highest accuracy possible.\n",
    "\n",
    "## Data\n",
    "\n",
    "The data used for this project comes from the MNIST dataset which is a very good basis for people who want to try learning techniques and pattern recognition method on real-world data.\n",
    "\n",
    "The data contains contains 70,000 instances of labeled images (28x28 pixels) representing handwritten digits going from 0 to 9.\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "These image-label pairs are divided into 60,000 training examples and 10,000 testing examples.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "For this project, the classification problem is tackled via Supervised Deep Learning. \n",
    "First of all, a data pre-processing step will be performed to explore the MNIST dataset and prepare data for analysis via encoding, formatting, and normalizing.\n",
    "Then, two neural networks will be built: \n",
    "\n",
    "* Multi-Layer Perceptron (MLP)\n",
    "* Convolutional Neural Network (CNN)\n",
    "\n",
    "Models' hyperparameters configuration will be tuned to maximize performances while preventing classic issues such as overfitting. \n",
    "Results will be finally compared to identify which model yields the highest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-rochester",
   "metadata": {},
   "source": [
    "# Modules Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "enclosed-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-athens",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alone-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beginning-mapping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "removed-hepatitis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train data</th>\n",
       "      <th>Test data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5923</td>\n",
       "      <td>980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6742</td>\n",
       "      <td>1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5958</td>\n",
       "      <td>1032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6131</td>\n",
       "      <td>1010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5842</td>\n",
       "      <td>982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5421</td>\n",
       "      <td>892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5918</td>\n",
       "      <td>958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6265</td>\n",
       "      <td>1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5851</td>\n",
       "      <td>974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5949</td>\n",
       "      <td>1009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Train data  Test data\n",
       "0        5923        980\n",
       "1        6742       1135\n",
       "2        5958       1032\n",
       "3        6131       1010\n",
       "4        5842        982\n",
       "5        5421        892\n",
       "6        5918        958\n",
       "7        6265       1028\n",
       "8        5851        974\n",
       "9        5949       1009"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = pd.Series(mnist_train_labels).value_counts()\n",
    "s2 = pd.Series(mnist_test_labels).value_counts()\n",
    "overview = pd.concat([s1.rename('Train data'), s2.rename('Test data')], axis=1)\n",
    "overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lovely-damage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  49, 125, 168, 254, 254,\n",
       "        254, 254, 160,  38,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0, 163, 250, 253, 253, 253, 253,\n",
       "        253, 253, 253, 205,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0, 106, 252, 253, 253, 253, 253, 253,\n",
       "        253, 253, 253, 251,  80,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  66, 246, 253, 253, 218, 149, 149,\n",
       "        176, 253, 253, 253, 205,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0, 144, 155,  42,  17,   0,   0,\n",
       "        116, 253, 253, 235,  65,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 177,\n",
       "        246, 253, 253, 207,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  73, 215, 246,\n",
       "        253, 253, 253,  97,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 192, 253, 253,\n",
       "        253, 253, 253, 199,  26,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 215, 253, 253,\n",
       "        253, 253, 253, 253, 219, 110,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 115, 253, 253,\n",
       "        253, 253, 253, 253, 253, 246, 175,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  22,  65,  65,\n",
       "         65,  95, 194, 235, 253, 253, 247, 108,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,  50, 166, 253, 253, 234,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,  12, 190, 253, 247,  77,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0, 163, 253, 253, 110,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   6, 175, 253, 253, 110,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  24,  27,  18,   0,   0,\n",
       "          0,   0,   0,   5, 128, 253, 253, 251,  99,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0, 228, 253, 219, 150, 150,\n",
       "         50,  20,  47, 169, 253, 253, 253, 223,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0, 222, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 253, 253, 232,  45,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  45, 117, 227, 249, 253,\n",
       "        253, 253, 253, 253, 252, 162,  45,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 159,\n",
       "        253, 253, 253, 230, 104,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train_images[298]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-campbell",
   "metadata": {},
   "source": [
    "Data pre-processing steps:\n",
    "\n",
    "1. One-hot encoding - labels converted into one-hot format (optimal format for classification models to handle labels over training). \n",
    "\n",
    "\n",
    "2. Reshaping images - depending on the model used: \n",
    "\n",
    "==> MLP: flat arrays of 784 values.\n",
    "\n",
    "==> CNN: keep image size of 28x28 pixels and add channel color value \"1\" consistently (depending on the Keras data format set up). N.B: as images are just in grayscale here, there is one single color channel (\"1\"). If we were dealing with color images, we'd have three color channels (red, green and blue). \n",
    "\n",
    "3. Normalization - convert images values into floating points and normalize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "communist-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP data pre-processing\n",
    "\n",
    "def process_MLP(train_images, train_labels, test_images, test_labels):\n",
    "    \n",
    "    # Labels one-hot encoding\n",
    "    train_labels = tensorflow.keras.utils.to_categorical(train_labels, 10)\n",
    "    test_labels = tensorflow.keras.utils.to_categorical(test_labels, 10)\n",
    "    \n",
    "    # Flatten 28x28 images into 784 values arrays\n",
    "    train_images = train_images.reshape(60000, 784)\n",
    "    test_images = test_images.reshape(10000, 784)\n",
    "    \n",
    "    # Normalization\n",
    "    train_images = train_images.astype('float32')\n",
    "    test_images = test_images.astype('float32')\n",
    "    train_images /= 255\n",
    "    test_images /= 255\n",
    "    \n",
    "    return train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "great-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN data pre-processing\n",
    "\n",
    "def process_CNN(train_images, train_labels, test_images, test_labels):\n",
    "    \n",
    "    # Labels one-hot encoding\n",
    "    train_labels = tensorflow.keras.utils.to_categorical(mnist_train_labels, 10)\n",
    "    test_labels = tensorflow.keras.utils.to_categorical(mnist_test_labels, 10)\n",
    "    \n",
    "    # Add chanel color value \"1\", depending on the Keras data format set up \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        train_images = train_images.reshape(mnist_train_images.shape[0], 1, 28, 28)\n",
    "        test_images = test_images.reshape(mnist_test_images.shape[0], 1, 28, 28)\n",
    "        input_shape = (1, 28, 28)\n",
    "    else:\n",
    "        train_images = mnist_train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "        test_images = mnist_test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
    "        input_shape = (28, 28, 1)\n",
    "        \n",
    "    # Normalization\n",
    "    train_images = train_images.astype('float32')\n",
    "    test_images = test_images.astype('float32')\n",
    "    train_images /= 255\n",
    "    test_images /= 255\n",
    "    \n",
    "    return train_images, train_labels, test_images, test_labels, input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-system",
   "metadata": {},
   "source": [
    "As a sanity check, let's print out one of the training example with its label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "persistent-absence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_images, train_labels, test_images, test_labels = process_MLP(mnist_train_images, mnist_train_labels, \n",
    "                                                                   mnist_test_images, mnist_test_labels)\n",
    "\n",
    "def display_sample(train_images, train_labels, num):\n",
    "    # Print the one-hot array of this sample's label \n",
    "    print(train_labels[num])  \n",
    "    # Print the image and label converted back to a number\n",
    "    image = train_images[num].reshape([28,28])\n",
    "    label = train_labels[num].argmax(axis=0)\n",
    "    plt.title('Sample: %d  Label: %d' % (num, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()\n",
    "    \n",
    "display_sample(train_images, train_labels, 1235)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-refund",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-theology",
   "metadata": {},
   "source": [
    "Network topology:\n",
    "\n",
    "* Input layer - 784 features feeds into a ReLU layer of 512 units\n",
    "* Dropout - apply dropout filter to prevent overfitting.\n",
    "* Hidden layer (optional) - ReLU layer of 128 units\n",
    "* Output layer - 10 units with softmax applied to choose our category of 0-9.\n",
    "\n",
    "We will use the RMSProp optimizer here. Other choices include Adagrad, SGD, Adam, Adamax, and Nadam. See https://keras.io/optimizers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "positive-boston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def MLP():\n",
    "    model = Sequential()\n",
    "    # Input layer\n",
    "    model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "    # Dropout\n",
    "    model.add(Dropout(0.5))\n",
    "    # Hidden layer (turns out to be inefficient)\n",
    "    model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "    # Output layer\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = MLP()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-charles",
   "metadata": {},
   "source": [
    "Let's train our MLP on 10 epochs with batch size of 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "expensive-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing\n",
    "train_images, train_labels, test_images, test_labels = process_MLP(mnist_train_images, mnist_train_labels, \n",
    "                                                                   mnist_test_images, mnist_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "floppy-alabama",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "600/600 - 7s - loss: 0.2822 - accuracy: 0.9134 - val_loss: 0.1127 - val_accuracy: 0.9650\n",
      "Epoch 2/10\n",
      "600/600 - 6s - loss: 0.1415 - accuracy: 0.9578 - val_loss: 0.0948 - val_accuracy: 0.9738\n",
      "Epoch 3/10\n",
      "600/600 - 7s - loss: 0.1167 - accuracy: 0.9651 - val_loss: 0.0800 - val_accuracy: 0.9775\n",
      "Epoch 4/10\n",
      "600/600 - 7s - loss: 0.1006 - accuracy: 0.9702 - val_loss: 0.0763 - val_accuracy: 0.9792\n",
      "Epoch 5/10\n",
      "600/600 - 8s - loss: 0.0928 - accuracy: 0.9739 - val_loss: 0.0691 - val_accuracy: 0.9814\n",
      "Epoch 6/10\n",
      "600/600 - 8s - loss: 0.0847 - accuracy: 0.9764 - val_loss: 0.0682 - val_accuracy: 0.9801\n",
      "Epoch 7/10\n",
      "600/600 - 7s - loss: 0.0786 - accuracy: 0.9779 - val_loss: 0.0852 - val_accuracy: 0.9785\n",
      "Epoch 8/10\n",
      "600/600 - 6s - loss: 0.0743 - accuracy: 0.9796 - val_loss: 0.0690 - val_accuracy: 0.9828\n",
      "Epoch 9/10\n",
      "600/600 - 7s - loss: 0.0719 - accuracy: 0.9804 - val_loss: 0.0765 - val_accuracy: 0.9809\n",
      "Epoch 10/10\n",
      "600/600 - 6s - loss: 0.0689 - accuracy: 0.9813 - val_loss: 0.0758 - val_accuracy: 0.9820\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels,\n",
    "                    batch_size=100,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "atlantic-reconstruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0757911279797554\n",
      "Test accuracy: 0.9819999933242798\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multi-Layer Perceptron</td>\n",
       "      <td>0.982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  Score\n",
       "0  Multi-Layer Perceptron  0.982"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "scores = pd.DataFrame(columns=['Model', 'Score'])\n",
    "scores.loc[0] = ['Multi-Layer Perceptron', score[1]]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-talent",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-slovenia",
   "metadata": {},
   "source": [
    "Setting up a convolutional neural network involves more layers. Not all of these are strictly necessary (eg. pooling and dropout) but those extra steps help avoid overfitting and run operations faster).\n",
    "\n",
    "Network topology:\n",
    "\n",
    "* Convolution 1 - 2D convolution of the image (set up to take 32 windows/filters of each image, each filter being 3x3 in size).\n",
    "\n",
    "* Convolution 2 - second convolution on top of that with 64 3x3 windows - this topology is just what comes recommended within Keras's own examples. Again you want to re-use previous research whenever possible while tuning CNN's, as it is hard to do.\n",
    "\n",
    "* Pooling - apply a MaxPooling2D layer that takes the maximum of each 2x2 result to distill the results down into something more manageable.\n",
    "\n",
    "* Dropout - apply dropout filter to prevent overfitting.\n",
    "\n",
    "* Flatten layer - flatten the 2D layer we have at this stage into a 1D layer. So at this point we can just pretend we have a traditional multi-layer perceptron...\n",
    "\n",
    "* Hidden layer - feed results into a hidden, flat layer of 128 units.\n",
    "\n",
    "* Dropout - apply dropout again to further prevent overfitting.\n",
    "\n",
    "* Output layer - feed results into a final 10 units layer where softmax is applied. \n",
    "\n",
    "This time, we use the Adam optimizer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sought-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels, test_images, test_labels, input_shape = process_CNN(mnist_train_images, mnist_train_labels, \n",
    "                                                                                mnist_test_images, mnist_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "indian-costume",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def CNN(input_shape, optimizer):\n",
    "    model = Sequential()\n",
    "    # Convolution 1 - 32 3x3 kernels\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    # Convolution 2 - 64 3x3 kernels\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    # Pooling - Reduce by taking the max of each 2x2 block\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # Dropout to avoid overfitting\n",
    "    model.add(Dropout(0.25))\n",
    "    # Flatten the results to one dimension \n",
    "    model.add(Flatten())\n",
    "    # Hidden layer \n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    # Dropout\n",
    "    model.add(Dropout(0.5))\n",
    "    # Output layer - Final categorization from 0-9 with softmax\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = CNN(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "manufactured-cholesterol",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 - 90s - loss: 0.1967 - accuracy: 0.9411 - val_loss: 0.0421 - val_accuracy: 0.9870\n",
      "Epoch 2/10\n",
      "1875/1875 - 90s - loss: 0.0811 - accuracy: 0.9757 - val_loss: 0.0350 - val_accuracy: 0.9881\n",
      "Epoch 3/10\n",
      "1875/1875 - 94s - loss: 0.0639 - accuracy: 0.9805 - val_loss: 0.0315 - val_accuracy: 0.9892\n",
      "Epoch 4/10\n",
      "1875/1875 - 97s - loss: 0.0507 - accuracy: 0.9847 - val_loss: 0.0295 - val_accuracy: 0.9913\n",
      "Epoch 5/10\n",
      "1875/1875 - 91s - loss: 0.0420 - accuracy: 0.9870 - val_loss: 0.0308 - val_accuracy: 0.9899\n",
      "Epoch 6/10\n",
      "1875/1875 - 90s - loss: 0.0400 - accuracy: 0.9878 - val_loss: 0.0353 - val_accuracy: 0.9900\n",
      "Epoch 7/10\n",
      "1875/1875 - 95s - loss: 0.0351 - accuracy: 0.9891 - val_loss: 0.0269 - val_accuracy: 0.9914\n",
      "Epoch 8/10\n",
      "1875/1875 - 94s - loss: 0.0291 - accuracy: 0.9907 - val_loss: 0.0314 - val_accuracy: 0.9914\n",
      "Epoch 9/10\n",
      "1875/1875 - 83s - loss: 0.0272 - accuracy: 0.9916 - val_loss: 0.0311 - val_accuracy: 0.9921\n",
      "Epoch 10/10\n",
      "1875/1875 - 84s - loss: 0.0260 - accuracy: 0.9915 - val_loss: 0.0339 - val_accuracy: 0.9904\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                    validation_data=(test_images, test_labels))\n",
    "\n",
    "score = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "scores.loc[len(scores)] = ['Convolutional Neural Network', score[1]]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "unexpected-mining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.033878814429044724\n",
      "Test accuracy: 0.9904000163078308\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multi-Layer Perceptron</td>\n",
       "      <td>0.9820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "      <td>0.9904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "      <td>0.9904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model   Score\n",
       "0        Multi-Layer Perceptron  0.9820\n",
       "1  Convolutional Neural Network  0.9904\n",
       "2  Convolutional Neural Network  0.9904"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "scores.loc[1] = ['Convolutional Neural Network', score[1]]\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
